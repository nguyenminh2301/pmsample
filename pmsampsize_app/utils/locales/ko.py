KO = {
        "title": "ì˜ˆí›„ ì—°êµ¬ í‘œë³¸ í¬ê¸° ë„êµ¬",
        "sidebar_title": "ì„¤ì •",
        "language": "ì–¸ì–´ / Language",
        "mode": "ë°©ë²• ì„ íƒ",
        "mode_riley": "ë°©ë²• 1: Riley ë“± (ë¶„ì„ì )",
        "mode_bayes": "ë°©ë²• 2: ë² ì´ì§€ì•ˆ ë³´ì¦ (ì‹œë®¬ë ˆì´ì…˜)",
        "mode_single": "ë‹¨ì¼ ì‹œë‚˜ë¦¬ì˜¤",
        "mode_batch": "ë¯¼ê°ë„ ë¶„ì„ (ë²”ìœ„)",
        "method1_tab": "ë°©ë²• 1 (Riley)",
        "method2_tab": "ë°©ë²• 2 (Bayesian)",
        "nav_title": "íƒìƒ‰",
        "nav_readme": "ìƒì„¸ ë¬¸ì„œ (README)",
        "nav_intro": "ì†Œê°œ ë° ê³µì‹",
        "nav_calc": "í‘œë³¸ í¬ê¸° ê³„ì‚°ê¸°",
        "intro_heading": "í™˜ì˜í•©ë‹ˆë‹¤",
        "intro_text": "ì´ ë„êµ¬ëŠ” ì´ë¶„í˜• ê²°ê³¼ê°€ ìˆëŠ” ì„ìƒ ì˜ˆì¸¡ ëª¨ë¸ ê°œë°œì— í•„ìš”í•œ ìµœì†Œ í‘œë³¸ í¬ê¸°ë¥¼ ê³„ì‚°í•˜ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤.",
        "formula_heading": "ìˆ˜í•™ì  í”„ë ˆì„ì›Œí¬ (ë°©ë²• 1)",
        "formula_intro": "ë°©ë²• 1ì€ Riley ë“±ì´ ì œê³µí•œ íì‡„í˜• ì†”ë£¨ì…˜ì„ ì‚¬ìš©í•˜ê³ , ë°©ë²• 2ëŠ” ë² ì´ì§€ì•ˆ MCMC ì‹œë®¬ë ˆì´ì…˜ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.",
        "sens_guide_title": "ğŸ’¡ ë¯¼ê°ë„ ë¶„ì„(ë°°ì¹˜ ëª¨ë“œ) ì‚¬ìš©ë²•",
        "sens_guide_text": """
        - **ë²”ìœ„**: `min-max` í˜•ì‹ìœ¼ë¡œ ì…ë ¥ (ì˜ˆ: `0.05-0.10`). ë‹¨ê³„ê°€ ìë™ìœ¼ë¡œ ìƒì„±ë©ë‹ˆë‹¤.
        - **íŠ¹ì • ê°’**: ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ëª©ë¡ ì…ë ¥ (ì˜ˆ: `0.05, 0.10, 0.15`).
        """,
        "detail_view": "ì‹œë‚˜ë¦¬ì˜¤ë³„ ìƒì„¸ ê³„ì‚° ë³´ê¸°",
        "footer_refs": "ì°¸ê³ ë¬¸í—Œ: Riley et al. (2018, 2020), BayesAssurance.",
        "calc_btn": "ê³„ì‚°í•˜ê¸°",
        "results": "ê²°ê³¼",
        "sanity": "ê±´ì „ì„± ì‹¬ì‚¬ (EPV ê·œì¹™)",
        "download_csv": "CSV ë‹¤ìš´ë¡œë“œ",
        "download_report": "ì „ì²´ ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ",
        "error_p": "ìœ ë³‘ë¥ ì€ 0ê³¼ 1 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤.",
        "error_auc": "AUCëŠ” 0.5ì™€ 1 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤.",
        "error_parse": "ì…ë ¥ì„ êµ¬ë¬¸ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
        "riley_inputs": "ì…ë ¥ íŒŒë¼ë¯¸í„° (Riley)",
        "prevalence": "ê²°ê³¼ ìœ ë³‘ë¥  (ì´ë²¤íŠ¸ ë°œìƒë¥ )",
        "prevalence_help": "ì´ë²¤íŠ¸ê°€ ë°œìƒí•œ ì°¸ê°€ìì˜ ë¹„ìœ¨ (0 < p < 1).",
        "parameters": "ì˜ˆì¸¡ ë³€ìˆ˜ íŒŒë¼ë¯¸í„° ìˆ˜ (df)",
        "parameters_help": "ì´ ììœ ë„ (ì ˆí¸ ì œì™¸).",
        "shrinkage": "ëª©í‘œ ê¸€ë¡œë²Œ ìˆ˜ì¶• (S)",
        "shrinkage_help": "í¬ë§í•˜ëŠ” ìˆ˜ì¶• ê³„ìˆ˜ (ê¸°ë³¸ê°’ 0.9).",
        "perf_measure": "ì˜ˆìƒ ì„±ëŠ¥",
        "perf_auc": "AUC (C-í†µê³„ëŸ‰)",
        "perf_r2": "Cox-Snell R-ì œê³±",
        "perf_cons": "ë³´ìˆ˜ì  ì ìˆ˜ (ìµœëŒ€ R2ì˜ 15%)",
        "bayes_inputs": "ì‹œë®¬ë ˆì´ì…˜ ì„¤ì • (ë² ì´ì§€ì•ˆ ë³´ì¦)",
        "dgm_settings": "ë°ì´í„° ìƒì„± ë§¤ì»¤ë‹ˆì¦˜",
        "sim_settings": "ì‹œë®¬ë ˆì´ì…˜ ë° MCMC",
        "eval_settings": "í‰ê°€ ê¸°ì¤€",
        "n_candidates": "í›„ë³´ í‘œë³¸ í¬ê¸° (ì‰¼í‘œë¡œ êµ¬ë¶„)",
        "n_candidates_help": "í…ŒìŠ¤íŠ¸í•  N ê°’ ëª©ë¡, ì˜ˆ: 500, 1000, 1500.",
        "correlation": "ì˜ˆì¸¡ ë³€ìˆ˜ ìƒê´€ê´€ê³„ (rho)",
        "n_sims": "Në‹¹ ì‹œë®¬ë ˆì´ì…˜ íšŸìˆ˜",
        "assurance_threshold": "ë³´ì¦ ì„ê³„ê°’ (ëª©í‘œ í™•ë¥ )",
        "run_simulation": "ë² ì´ì§€ì•ˆ ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰",
        "simulation_running": "ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰ ì¤‘... ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        "assurance_result": "ë³´ì¦ ë¶„ì„",
        "mode_dev_sim": "ë°©ë²• 6: ê°œë°œ ì‹œë®¬ë ˆì´ì…˜ (ë¹ˆë„ì£¼ì˜)",
        "method6_tab": "ë°©ë²• 6 (ì‹œë®¬ë ˆì´ì…˜)",
        "dev_sim_intro": "ëª¨ë¸ ê°œë°œì„ ìœ„í•œ ì‹œë®¬ë ˆì´ì…˜ ê¸°ë°˜ í‘œë³¸ í¬ê¸° (samplesizedevì™€ ìœ ì‚¬í•œ ë¹ˆë„ì£¼ì˜ ì ‘ê·¼ë²•).",
        "dev_mode_simple": "ëª¨ë“œ A: ë‹¨ìˆœ (AUC ê¸°ë°˜)",
        "dev_mode_custom": "ëª¨ë“œ B: ì‚¬ìš©ì ì •ì˜ DGM",
        "target_auc": "ëª©í‘œ í‰ê·  AUC (C-í†µê³„ëŸ‰)",
        "target_auc_help": "ì•Œê³ ë¦¬ì¦˜ì´ ì´ AUCë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•œ ë² íƒ€ ê³„ìˆ˜ë¥¼ ì°¾ìŠµë‹ˆë‹¤.",
        "criteria_settings": "ì„±ëŠ¥ ê¸°ì¤€ (í†µê³¼/ì‹¤íŒ¨)",
        "crit_slope_mean": "í‰ê·  êµì • ê¸°ìš¸ê¸° >= 0.9",
        "crit_slope_ci": "Pr(0.9 <= ê¸°ìš¸ê¸° <= 1.1) >= 80%",
        "crit_auc": "í‰ê·  AUC >= ëª©í‘œ",
        "audit_trail": "RNG ê°ì‚¬ ì¶”ì  (JSON)",
        "future_methods": "í–¥í›„ ë²„ì „ì—ì„œ ì œê³µ ì˜ˆì •...",
        "method_quick_tab": "A. ì‹ ì† / ê¸°ë³¸",
        "quick_mode_epv": "A1: EPV / EPP ê·œì¹™ (ê²½í—˜ì )",
        "quick_mode_risk": "A2: ê¸°ë³¸ ìœ„í—˜ ì •ë°€ë„ (CI ë„ˆë¹„)",
        "target_epv": "ëª©í‘œ íŒŒë¼ë¯¸í„°ë‹¹ ì´ë²¤íŠ¸ ìˆ˜ (EPP)",
        "target_epv_help": "ì¼ë°˜ì ì¸ ê²½í—˜ì  ìˆ˜ì¹˜ëŠ” 10, 15, 20ì…ë‹ˆë‹¤. EPPê°€ EPVë³´ë‹¤ ì„ í˜¸ë©ë‹ˆë‹¤.",
        "epv_warning_title": "âš ï¸ ì¤‘ìš” ê²½ê³ ",
        "epv_warning_text": "EPV/EPPëŠ” ëŒ€ëµì ì¸ ê²½í—˜ì  ê·œì¹™ì…ë‹ˆë‹¤. êµì •, íŒë³„ì„ ë³´ì¥í•˜ê±°ë‚˜ ë‚™ê´€ì£¼ì˜ë¥¼ ë°©ì§€í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë³€ìˆ˜ ì„ íƒ ë° ë¹„ì„ í˜• í•­ì— ë¯¼ê°í•©ë‹ˆë‹¤.",
        "ci_level": "ì‹ ë¢° ìˆ˜ì¤€",
        "ci_half_width": "ëª©í‘œ ë°˜-ë„ˆë¹„ (ì˜¤ì°¨ í•œê³„)",
        "ci_method": "CI ë°©ë²•",
        "ci_method_wilson": "Wilson Score (ê¶Œì¥)",
        "ci_method_wald": "Wald (ë‹¨ìˆœ)",
        "ci_method_cp": "Clopper-Pearson (ë³´ìˆ˜ì )",
        "risk_help": "íŠ¹ì • ì •ë°€ë„ë¡œ ì´ë²¤íŠ¸ ë°œìƒë¥  pë¥¼ ì¶”ì •í•˜ê¸° ìœ„í•œ Nì„ ê³„ì‚°í•©ë‹ˆë‹¤. ì˜ˆì¸¡ ëª¨ë¸ ì„±ëŠ¥ì„ ë³´ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
        "title_b3": "B3: ë¡œì§€ìŠ¤í‹± ê²€ì •ë ¥ (Hsieh)",
        "title_b4": "B4: Cox ê²€ì •ë ¥ (Schoenfeld)",
        "interpretation": "ê²°ê³¼ í•´ì„",
        "d8_assumptions": "**ê°€ì •**: Hanley & McNeil (1982) ë¶„ì‚° ê·¼ì‚¬ì¹˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. AUCì— ëŒ€í•´ ëŒ€ì¹­ì  ì •ê·œ ë¶„í¬ë¥¼ ê°€ì •í•©ë‹ˆë‹¤.",
        "d8_mode_n_to_width": "Nìœ¼ë¡œë¶€í„° CI ë„ˆë¹„ ê³„ì‚°",
        "d8_mode_width_to_n": "CI ë„ˆë¹„ë¡œë¶€í„° í•„ìš”í•œ N ê³„ì‚°",
        "d8_opt_settings": "ê³ ê¸‰ ìµœì í™” ì„¤ì •",
        "d8_practical_rounding": "ì‹¤ìš©ì ì¸ ì •ìˆ˜ ë°˜ì˜¬ë¦¼ í‘œì‹œ",
        "d8_n_input": "í‘œë³¸ í¬ê¸° (N)",
        "d8_width_input": "CI ë„ˆë¹„ (í•©ê³„)",
        "d8_opt_bound": "íƒìƒ‰ ìƒí•œì„ ",
        "d8_opt_tol": "í—ˆìš© ì˜¤ì°¨",
        "title_d8": "D8: AUC ì •ë°€ë„ (Hanley-McNeil)",
        "d8_desc": "í¬ë§í•˜ëŠ” ì •ë°€ë„(CI ë„ˆë¹„)ë¡œ AUCë¥¼ ì¶”ì •í•˜ê¸° ìœ„í•œ í‘œë³¸ í¬ê¸°ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.",
        "auc_expected": "ì˜ˆìƒ AUC (C-í†µê³„ëŸ‰)",
        "formulas_header": "ğŸ“š ê³µì‹ ë° ê¸°ìˆ ì  ì„¸ë¶€ ì‚¬í•­",
        "title_d9": "D9: ì™¸ë¶€ ê²€ì¦ (ë§ì¶¤í˜•)",
        "common_inputs": "ê³µí†µ íŒŒë¼ë¯¸í„°",
        "search_placeholder": "ë°©ë²• ê²€ìƒ‰...",
        "settings": "ì„¤ì •",
        "footer_copyright": "Â© 2026 Prognostic Research Sample Size Tool. í•™ìˆ /ì—°êµ¬ìš© ì „ìš©.",
        "footer_author": "ì €ì ë° ìœ ì§€ê´€ë¦¬: Minh Nguyen (minhnt@ump.edu.vn)",
        "footer_disclaimer": "ë©´ì±… ì¡°í•­: ì„ìƒì  ë³´ì¦ ì—†ìŒ. ì‚¬ìš©ìëŠ” ê²°ê³¼ ê²€ì¦ ë° í•´ì„ì— ëŒ€í•œ ì±…ì„ì´ ìˆìŠµë‹ˆë‹¤.",

        "intro_complete_md": """
### Welcome

This app helps clinicians and researchers plan minimum sample size for prognostic research, including:
* Prognostic factor studies (power to detect associations),
* Clinical prediction model development (risk prediction), and
* Model validation / updating (external validation, recalibration).

It is designed for binary outcomes (e.g., event vs no event) and, for some modules, time-to-event outcomes (Cox PH).

Source code (download): [https://gitlab.com/minhthiennguyen/pmsample/](https://gitlab.com/minhthiennguyen/pmsample/)

### Getting started (for new users)

#### 1. Clarify your study goal
* Are you testing a single prognostic factor (association)?
* Are you building a prediction model?
* Are you validating an existing model in a new population?

#### 2. Estimate the event rate $p$ (or event fraction for survival)
* Prefer local hospital data (best).
* If uncertain, enter a range and run a sensitivity analysis.

#### 3. Count model complexity correctly (parameters / df)
Use parameters (degrees of freedom), not just "number of variables."
* Binary predictor: 1 df
* Categorical with $L$ levels: $L-1$ df
* Spline (RCS with $K$ knots): $K-1$ df
* Interaction: $df(A \\times B) = df(A) \\cdot df(B)$

#### 4. Choose a method from the catalog below
* Use **"Quick tools"** for rough planning only.
* Use **Riley / simulation / assurance** when you are developing a prediction model.

---

### When to use this app (and when not to)

**Use this app when you are:**
* Planning retrospective or prospective cohort studies in prognosis/prediction
* Developing or validating risk prediction models
* Estimating sample size for precision (CI width) of prevalence or AUC
* Designing external validation with calibration and discrimination targets

**Do NOT use this app as the primary tool when you are:**
* Designing randomized controlled trials (use RCT-specific power/sample size methods)
* Planning diagnostic accuracy studies for sensitivity/specificity without prediction modeling
* Expecting a single "correct" number: sample size planning requires assumptions and should include sensitivity analyses

---

### Available Methods (Overview)

#### A. Quick / Basic (fast, approximate)

**A1 â€” Rules of Thumb (EPV/EPP) (heuristic)**
* **Use when:** you need a quick sanity check on whether events are "roughly sufficient" for a planned model size.
* **Do not use when:** model includes splines/interactions/variable selection, or event rate is lowâ€”EPV/EPP does not guarantee good calibration or low optimism.
* **Key inputs:** event rate $p$, number of parameters $P$ (df), target EPP (e.g., 10/15/20)
* **Core output:** required events $E=t \\cdot P$, required sample size $N=\\lceil E/p \\rceil$
* **Strengths:** extremely simple; good for early feasibility
* **Weaknesses:** can be misleading; not performance-based

**A2 â€” Baseline Risk Precision (CI width for prevalence)**
* **Use when:** your goal is to estimate the event rate $p$ with a desired CI half-width (e.g., Â±2%).
* **Do not use when:** you want prediction model performance guarantees (AUC/calibration slope).
* **Key inputs:** expected $p$, CI method (Wilson recommended), confidence level, target half-width $d$
* **Core output:** minimum $N$ such that CI half-width $\\le d$
* **Strengths:** direct precision target; transparent assumptions
* **Weaknesses:** about prevalence only, not model performance

#### B. Prognostic factor (power) (association-focused, not prediction model sizing)

**B3 â€” Logistic OR Power (Hsieh)**
* **Use when:** you want power to detect a target odds ratio (OR) for a prognostic factor in logistic regression.
* **Do not use when:** your primary goal is prediction model development (calibration/discrimination), not hypothesis testing.
* **Key inputs:** baseline risk $p_0$, target OR, alpha, power, exposure prevalence (binary) or SD (continuous), optional $R^2$ with covariates
* **Core output:** required $N$ (and implied events) to detect the OR
* **Strengths:** classic power framework for association
* **Weaknesses:** does not address prediction model performance; sensitive to input assumptions

**B4 â€” Cox HR Power (Schoenfeld)**
* **Use when:** time-to-event outcome; you want power to detect a hazard ratio (HR) under Cox PH.
* **Do not use when:** PH assumption likely violated, or event fraction is highly uncertain and cannot be reasonably estimated.
* **Key inputs:** HR, alpha, power, allocation proportion (binary) or SD (continuous), expected event fraction during follow-up
* **Core output:** required number of events; convert to $N$ using event fraction
* **Strengths:** widely accepted; event-based planning is intuitive
* **Weaknesses:** depends strongly on event fraction and follow-up/censoring assumptions

#### C. Prediction model development (recommended for risk model building)

**C5 â€” Riley et al. (Analytical; pmsampsize-like)**
* **Use when:** developing a multivariable prediction model; you want to control overfitting and ensure adequate precision.
* **Do not use when:** you cannot provide reasonable assumptions for prevalence and anticipated model performance (AUC or $R^2$); in that case, use sensitivity analysis or simulation.
* **Key inputs:** event rate $p$, parameters $P$ (df), target shrinkage (e.g., 0.90), anticipated model performance (AUC or Coxâ€“Snell $R^2$)
* **Core output:** minimum $N$ meeting multiple criteria (overfitting control + precision)
* **Strengths:** principled, performance-aware, widely cited
* **Weaknesses:** depends on performance assumptions; requires careful df counting

**C6 â€” Development Simulation (Frequentist; samplesizedev/custom DGM)**
* **Use when:** you prefer "simulate what you will do," especially with nonlinearity/interactions and custom data structures.
* **Do not use when:** you cannot specify a plausible data-generating mechanism (DGM) or you need results instantly (compute-intensive).
* **Key inputs:** candidate $N$ grid, DGM assumptions (predictor distributions/correlations/effects), performance targets (e.g., calibration slope range, AUC threshold), simulation replicates, seed
* **Core output:** smallest $N$ achieving targets with acceptable probability/precision
* **Strengths:** flexible; aligns with complex modeling
* **Weaknesses:** assumptions-heavy; computational cost

**C7 â€” Bayesian Assurance (MCMC)**
* **Use when:** the final model will be estimated with Bayesian MCMC, and you want sample size based on assurance (probability of meeting posterior performance/precision targets).
* **Do not use when:** priors cannot be justified or computation budget is limited.
* **Key inputs:** DGM, priors, candidate $N$, MCMC settings, assurance threshold (e.g., 80%/90%), performance/precision targets
* **Core output:** minimal $N$ meeting assurance threshold
* **Strengths:** coherent for Bayesian workflows; directly targets posterior criteria
* **Weaknesses:** computationally intensive; requires prior specification

#### D. Validation / Updating (for existing models)

**D8 â€” AUC Precision (Hanleyâ€“McNeil / presize)**
* **Use when:** your validation goal is precision of AUC (CI width).
* **Do not use when:** calibration (slope/CITL) is the primary concernâ€”this method targets AUC only.
* **Key inputs:** expected AUC, prevalence or case-control ratio, confidence level, target CI width
* **Core output:** minimum $N$ to achieve desired AUC CI width
* **Strengths:** simple; quick planning for discrimination precision
* **Weaknesses:** approximate variance; ignores calibration

**D9 â€” External Validation (Tailored; pmvalsampsize / sampsizeval)**
* **Use when:** you want validation sizing targeting multiple performance measures (calibration + discrimination), often requiring assumptions about the LP distribution.
* **Do not use when:** you cannot justify LP distribution assumptions or expected performance.
* **Key inputs:** prevalence, expected AUC, calibration slope/CITL targets, CI widths or SE targets, LP distribution assumptions
* **Core output:** recommended $N$ meeting precision criteria across measures
* **Strengths:** tailored; calibration-aware
* **Weaknesses:** requires additional assumptions; more complex

**D10 â€” External Validation (Simulation; LP-based)**
* **Use when:** you can specify/estimate the distribution of the linear predictor (LP) in the target validation population and want simulation-based precision planning.
* **Do not use when:** LP distribution is unknown and cannot be approximated.
* **Key inputs:** LP distribution (normal/beta/empirical), miscalibration parameters, CI width targets for metrics, replicates, seed
* **Core output:** minimal $N$ achieving precision targets under simulation
* **Strengths:** very flexible; matches "simulate what you expect"
* **Weaknesses:** assumptions-heavy; computational cost

**D11 â€” Updating / Recalibration (intercept/slope)**
* **Use when:** you will recalibrate an existing model (update intercept and/or slope) and need adequate precision.
* **Do not use when:** you are developing a brand-new model (use C5â€“C7).
* **Key inputs:** updating type (intercept only vs intercept+slope), event rate, precision targets
* **Core output:** $N$ sufficient for stable updating
* **Strengths:** practical for real-world deployment
* **Weaknesses:** depends on local case-mix and model transportability assumptions

---

#### disclaimer

No clinical warranty; users are responsible for validation and interpretation. Always document assumptions and run sensitivity analyses.

#### Contact

Author & Maintenance: Minh Nguyen (minhnt@ump.edu.vn)
""",

        "a2_content_md": """
### ì´ê²ƒì€ ë¬´ì—‡ì…ë‹ˆê¹Œ

ì´ ëª¨ë“ˆì€ **ì›í•˜ëŠ” ì •ë°€ë„**(ì‹ ë¢° êµ¬ê°„(CI) ë°˜í­ ë˜ëŠ” ì˜¤ì°¨ í•œê³„ë¡œ í‘œí˜„ë¨)ë¡œ **ê¸°ë³¸ ìœ„í—˜ / ì‚¬ê±´ ë°œìƒë¥ **(p)(ì¦‰, ê²°ê³¼ì˜ ìœ ë³‘ë¥ )ì„ ì¶”ì •í•˜ëŠ” ë° í•„ìš”í•œ **ìµœì†Œ í‘œë³¸ í¬ê¸°(n)**ë¥¼ ì¶”ì •í•©ë‹ˆë‹¤.

ë‹¤ìŒê³¼ ê°™ì€ ê²½ìš°ì— ìœ ìš©í•©ë‹ˆë‹¤:
* ì§€ì •ëœ ì •ë°€ë„ë¡œ ì½”í˜¸íŠ¸ ë‚´ ê²°ê³¼ ìœ ë³‘ë¥  ì„¤ëª…,
* íƒ€ë‹¹ì„± ê³„íš ë° ê¸°ë³¸ ìœ„í—˜ ë³´ê³ ,
* êµì • ê´€ë ¨ ê³„íš ì§€ì› (ì˜ˆ: calibration-in-the-largeëŠ” ì‚¬ê±´ ë°œìƒë¥ ì— ì˜ì¡´).

**ì¤‘ìš”í•œ ì œí•œ ì‚¬í•­:** ì´ ê³„ì‚°ì€ ì˜ˆì¸¡ ëª¨ë¸ ì„±ëŠ¥(AUC, êµì • ê¸°ìš¸ê¸°, ë‚™ê´€ì£¼ì˜)ì„ **ë³´ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤**. ì˜¤ì§ (p) ì¶”ì •ì˜ ì •ë°€ë„ë§Œì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.

---

### ì…ë ¥ ê°’ (ì˜ë¯¸)

1. **ê²°ê³¼ ìœ ë³‘ë¥  / ì‚¬ê±´ ë°œìƒë¥ ** (p)
   ëŒ€ìƒ ëª¨ì§‘ë‹¨ì—ì„œ ì˜ˆìƒë˜ëŠ” ì‚¬ê±´ ë¹„ìœ¨ (ì˜ˆ: 0.10).
   * ì•Œ ìˆ˜ ì—†ëŠ” ê²½ìš°, íƒ€ë‹¹í•œ ë²”ìœ„ë¥¼ ê³ ë ¤í•˜ì—¬ ë¯¼ê°ë„ ë¶„ì„ì„ ì‹¤í–‰í•˜ì‹­ì‹œì˜¤.
   * ìœ ë³‘ë¥  ì •ë°€ë„ì— ëŒ€í•´ ë³´ìˆ˜ì ì¸ "ìµœì•…ì˜ ê²½ìš°"ë¥¼ ì›í•œë‹¤ë©´ (p=0.50)ì„ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤ (ë¶„ì‚° ìµœëŒ€í™”).

2. **ëª©í‘œ ë°˜í­ (ì˜¤ì°¨ í•œê³„)** (d)
   CIê°€ ëŒ€ëµ ë‹¤ìŒê³¼ ê°™ë„ë¡ í•˜ëŠ” ì›í•˜ëŠ” ì •ë°€ë„:
   $p \pm d$
   ì˜ˆ: (d = 0.01, 0.02, 0.03) (ì¦‰, Â±1%, Â±2%, Â±3%).

3. **ì‹ ë¢° ìˆ˜ì¤€** (1-$\\alpha$)
   ì¼ë°˜ì ì¸ ê°’: 0.95 ë˜ëŠ” 0.99.

4. **CI ë°©ë²•**
* **Wilson score (ê¶Œì¥):** Waldë³´ë‹¤ ì»¤ë²„ë¦¬ì§€ê°€ ì¢‹ìœ¼ë©°, íŠ¹íˆ (p)ê°€ 0ì´ë‚˜ 1ì— ê°€ê¹ê±°ë‚˜ í‘œë³¸ í¬ê¸°ê°€ ì ë‹¹í•  ë•Œ ì¢‹ìŠµë‹ˆë‹¤.
* **Wald (ì •ê·œ ê·¼ì‚¬):** ê°„ë‹¨í•œ íì‡„í˜•ì´ì§€ë§Œ (n)ì´ ì‘ê±°ë‚˜ (p)ê°€ ê·¹ë‹¨ì ì¼ ë•Œ ì„±ëŠ¥ì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
* **Clopperâ€“Pearson (ì •í™•):** ë³´ìˆ˜ì ì…ë‹ˆë‹¤ (ì¢…ì¢… ë” ë„“ì€ CIë¥¼ ì‚°ì¶œí•˜ë¯€ë¡œ ë” í° (n)ì´ í•„ìš”í•¨).

---

### í•µì‹¬ ê³„ì‚° (ì›ë¦¬)

$X \sim \\text{Binomial}(n,p)$, $\hat p = X/n$ì´ë¼ê³  í•©ì‹œë‹¤. ëª©í‘œëŠ” ì„ íƒí•œ CI ë°©ë²•ì´ ë‹¤ìŒì„ ì‚°ì¶œí•˜ë„ë¡ í•˜ëŠ” ê°€ì¥ ì‘ì€ (n)ì„ ì°¾ëŠ” ê²ƒì…ë‹ˆë‹¤:
$$
\\frac{\\text{Upper}(n) - \\text{Lower}(n)}{2} \le d
$$

#### A) Wald (íì‡„í˜• ê·¼ì‚¬)
$$ n \\approx \\frac{z^2 p(1-p)}{d^2} $$
**ì°¸ê³ :** ë¹ ë¥´ì§€ë§Œ (n)ì´ ì‘ê±°ë‚˜ (p)ê°€ ê·¹ë‹¨ì ì¼ ë•ŒëŠ” ê¶Œì¥ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

#### B) Wilson score êµ¬ê°„ (ê¶Œì¥)
Wilson score êµ¬ê°„ ê³µì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

#### C) Clopperâ€“Pearson â€œì •í™•â€ êµ¬ê°„
Beta ë¶„ìœ„ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ë³´ìˆ˜ì ì¸ ë°©ë²•ì…ë‹ˆë‹¤.

---

### ì‹¤ìš©ì ì¸ ê¸°ë³¸ê°’

* **ì‹ ë¢° ìˆ˜ì¤€:** 95%ê°€ í‘œì¤€ì…ë‹ˆë‹¤.
* **ë°˜í­ (d):** Â±0.01 ~ Â±0.03 (1%â€“3%)ì´ ì¼ë°˜ì ì¸ ëª©í‘œì…ë‹ˆë‹¤.
* **ë°©ë²•:** Wilsonì´ ê°•ë ¥í•œ ê¸°ë³¸ê°’ì…ë‹ˆë‹¤.

### ì£¼ìš” ì°¸ê³  ë¬¸í—Œ
1. **Wilson EB.** Probable inference... *JASA.* 1927.
2. **Newcombe RG.** Two-sided confidence intervals... *Stat Med.* 1998.
""",

        "b3_content_md": """
### Purpose (what this method is)

This module estimates the **minimum sample size** needed to detect an association between a predictor (X) and a **binary outcome** (Y) using **logistic regression**, targeting a specified **odds ratio (OR)**, **two-sided ($\\alpha$)**, and **power**.

This is a **prognostic factor / association-focused** power calculation (testing a regression coefficient), **not** a prediction-model performance method. It does **not** guarantee good calibration or discrimination of a multivariable prediction model.

---

### When to use

Use B3 when:

* You want power to detect a **clinically meaningful OR** for a **single predictor** (binary or continuous) in logistic regression.
* Your primary goal is **hypothesis testing** (is the predictor associated with the outcome?), not building a risk prediction model.

### When NOT to use

Do not use B3 as your main approach when:

* Your goal is **prediction model development** (use Riley/pmsampsize or simulation/assurance methods).
* You plan **data-driven variable selection**, many interactions/splines, or complex machine-learning tuning (power for a single coefficient is not the right target).
* Data are **clustered** (multicenter/ward-level correlation) or strongly dependent without adjusting the design effect.
* You have a **caseâ€“control** design with fixed case/control sampling (baseline risks ($p_0$) may not represent the source population).

---

## Statistical model and parameters

Logistic regression model:
$$
\\text{logit}{P(Y=1\\mid X)}=\\beta_0+\\beta_1 X
$$

* For **binary** ($X\\in\\{0,1\\}$):
  $$
  \\mathrm{OR}=\\exp(\\beta_1)
  $$
* For **continuous** ($X$): OR must be defined for a specific change in ($X$), commonly **1 SD increase**.

Hypothesis test:
$$
H_0:\\beta_1=0 \\quad \\text{vs}\\quad H_1:\\beta_1\\neq 0
$$

---

## Inputs (what each value means)

1. **Alpha (two-sided)** ($\\alpha$)
   Common choices: 0.05 (standard), 0.01 (more stringent).

2. **Power** ($1-\\beta$)
   Common choices: 0.80 (standard), 0.90 (more conservative).

3. **Baseline event rate** ($p_0$)

   * For **binary predictor**: ($p_0 = P(Y=1\\mid X=0)$) (event rate in the reference group).
   * For **continuous predictor**: ($p_0$) is typically interpreted as the event rate at the **mean** of ($X$) (after centering).

4. **Target odds ratio** ($\\mathrm{OR}$)
   The smallest OR that is clinically meaningful and worth detecting.

5. **Predictor type**

* **Binary predictor**: requires **prevalence of (X=1)**, denoted ($q=P(X=1)$).
* **Continuous predictor**: typically requires the OR for a **1 SD increase** (or you must convert using SD).

6. **($R^2$) with other covariates**
   ($R^2$) is the squared multiple correlation from regressing ($X$) on other covariates in a multivariable model.

   * If ($X$) is correlated with other predictors, the effective information about ($\\beta_1$) decreases, so the required sample size increases.

---

# Calculation

## Step 1 â€” Convert OR and baseline risk to ($p_1$) (binary ($X$))

If ($X$) is binary, compute the event rate in the exposed group ($p_1=P(Y=1\\mid X=1)$) from ($p_0$) and OR:

$$
\\text{odds}_0=\\frac{p_0}{1-p_0},\\quad \\text{odds}_1=\\mathrm{OR}\\cdot \\text{odds}_0,\\quad
p_1=\\frac{\\text{odds}_1}{1+\\text{odds}_1}
$$

Overall event rate:
$$
p=(1-q)p_0+q p_1
$$

## Step 2 â€” Z-scores

Let:
$$
z_{\\alpha}=z_{1-\\alpha/2}, \\qquad z_{\\beta}=z_{1-\\beta}=z_{\\text{power}}
$$

## A) Binary predictor sample size (Hsieh approach)

With ($q=P(X=1)$), ($p_0=P(Y=1\\mid X=0)$), ($p_1=P(Y=1\\mid X=1)$), and ($p$) as above:

$$
n_0=
\\frac{
\\left[
z_{\\alpha}\\sqrt{\\frac{p(1-p)}{q(1-q)}}
+
z_{\\beta}\\sqrt{\\frac{p_1(1-p_1)}{q}+\\frac{p_0(1-p_0)}{1-q}}
\\right]^2
}
{(p_1-p_0)^2}
$$

### Adjustment for correlation with other covariates

If you plan a multivariable model and the predictor of interest ($X$) correlates with other covariates, inflate the sample size using:

$$
n=\\frac{n_0}{1-R^2}
$$

### Expected number of events

$$
E \\approx n\\cdot p
$$

---

## B) Continuous predictor sample size (Hsieh approach)

Assume a logistic model with a continuous predictor ($X$) and define OR for a **1 SD increase** in ($X$), denoted ($\\mathrm{OR}_{SD}$). Let ($p_0$) be the event rate at the mean of ($X$):

$$
n_0=\\frac{(z_{\\alpha}+z_{\\beta})^2}{p_0(1-p_0) [\\log(\\mathrm{OR}_{SD})]^2}
$$

If the user has an OR per 1-unit increase, ($\\mathrm{OR}_{unit}$), and SD of ($X$) is ($\\sigma_X$), convert:
$$
\\log(\\mathrm{OR}_{SD})=\\log(\\mathrm{OR}_{unit})\\cdot \\sigma_X
$$

Then apply the same multivariable correlation inflation:
$$
n=\\frac{n_0}{1-R^2}
$$

---

## Practical guidance: what values to choose (common conventions)

* **($\\alpha$)**: 0.05 (two-sided) is typical; use smaller ($\\alpha$) if multiple testing is expected.
* **Power**: 0.80 is common; 0.90 is preferred when missing the effect would be costly.
* **OR**: choose the **minimum clinically meaningful** OR (often in the 1.2â€“2.0 range depending on context).
* **Baseline risk ($p_0$)**: use local hospital/cohort data if available; otherwise use literature estimates and run sensitivity analyses.
* **Binary predictor prevalence ($q$)**: use local prevalence; note ($q$) near 0.5 gives the **largest information** (smaller ($n$)); very small/large ($q$) increases required ($n$).
* **($R^2$)**: if uncertain, run a sensitivity range (e.g., 0, 0.1, 0.25, 0.5). Even moderate correlation can inflate ($n$) substantially via ($1/(1-R^2)$).
* **Continuous predictors**: consider standardizing ($X$) to mean 0, SD 1 so ($\\mathrm{OR}_{SD}$) is easy to interpret.

---

## Key references (2â€“5)

1. Hsieh FY, Bloch DA, Larsen MD. *A simple method of sample size calculation for linear and logistic regression.* Statistics in Medicine. 1998;17(14):1623â€“1634.
2. Hsieh FY. *Sample size tables for logistic regression.* Statistics in Medicine. 1989;8(7):795â€“802.
3. Whittemore AS. *Sample size for logistic regression with small response probability.* Journal of the American Statistical Association. 1981;76:27â€“32.
""",
        "c5_content_md": """
### What this method is

C5 implements the **Riley et al. analytical minimum sample size criteria** for **developing a multivariable clinical prediction model** with a **binary outcome** (logistic regression). The goal is to ensure the development dataset is large enough to:

1. **Limit overfitting** (via a target global shrinkage / calibration slope),
2. Achieve **adequate precision** for model performance (via a bound on optimism in $R^2$), and
3. Estimate the **overall outcome risk** (intercept/baseline risk) with acceptable precision.

This is a **model development** method (not external validation). It is particularly suitable when you plan a **pre-specified model form** (predictors and coding defined in advance) and want a **principled alternative to EPV rules**.

---

### When to use

Use C5 when:

* You are **developing** a new prediction model for a **binary outcome**.
* You can specify (even approximately) the **event rate** and an anticipated **overall model performance** (Coxâ€“Snell $R^2$ or AUC).
* You want to target **low overfitting** (e.g., shrinkage $S \\ge 0.90$) and reasonable precision.

### When NOT to use (or use with caution)

Do not rely on C5 alone when:

* You will do extensive **data-driven variable selection**, multiple interactions/splines, or heavy ML tuning without adjusting the **effective number of parameters (df)**.
* Your data are strongly **clustered** (multicenter) without accounting for design effects.
* The intended modeling approach is not standard logistic regression (e.g., complex ML) unless you map complexity to an appropriate **effective df** or switch to simulation-based sizing.
* You cannot justify any plausible performance input (AUC/$R^2$); in that case run wide sensitivity analyses and consider simulation-based methods.

---

## Key inputs (what each means)

1. **Outcome prevalence / event rate** (p)
   Expected proportion with (Y=1) in the development dataset.

2. **Number of predictor parameters (df)** (P)
   Total degrees of freedom for all candidate predictors **excluding the intercept**.
   Include: dummy variables, spline bases, interactions (and any other basis expansions).

3. **Anticipated performance** (choose one)

* **Coxâ€“Snell ($R^2_{CS}$)**: preferred if available from related prior studies (ideally optimism-adjusted).
* **AUC (C-statistic)**: if $R^2_{CS}$ is unavailable, the tool can approximate $R^2_{CS}$ from AUC and ($p$) using a published approach.
* **Conservative (15% of max $R^2$)**: a fallback when neither AUC nor $R^2$ is available; use with caution.

4. **Target global shrinkage** (S)
   A target for **overall overfitting control** (often interpreted similarly to an expected calibration slope after internal validation).

* Common default: $S = 0.90$ ($\\approx$ 10% shrinkage of predictor effects).
* More conservative: $S = 0.95$ (requires larger sample size).

---

## Core concepts and formulas

### Coxâ€“Snell ($R^2$) and its maximum

Coxâ€“Snell ($R^2$) for a fitted logistic model can be written as:
$$
R^2_{CS} = 1-\\exp\\left(\\frac{2}{n}(\\ell_0-\\ell_1)\\right),
$$
where $\\ell_0$ is the intercept-only log-likelihood and $\\ell_1$ is the model log-likelihood.

For binary outcomes, $R^2_{CS}$ cannot reach 1. Its maximum depends on the outcome prevalence:
$$
\\ell_0 = n\\Big[p\\ln(p) + (1-p)\\ln(1-p)\\Big],
$$
$$
R^2_{CS,\\max}=1-\\exp\\left(\\frac{2\\ell_0}{n}\\right)
=1-\\exp\\Big(2[p\\ln(p) + (1-p)\\ln(1-p)]\\Big).
$$

Nagelkerke ($R^2$) rescales Coxâ€“Snell ($R^2$) to ([0,1]):
$$
R^2_{Nag}=\\frac{R^2_{CS}}{R^2_{CS,\\max}}.
$$

---

## The three Riley criteria (binary outcome)

### Criterion 1 â€” Control overfitting via target shrinkage (S)

Minimum sample size to target global shrinkage (S):
$$
n_1=\\left\\lceil
\\frac{P}{(S-1)\\ln\\left(1-\\frac{R^2_{CS}}{S}\\right)}
\\right\\rceil.
$$

### Criterion 2 â€” Limit optimism in ($R^2$) (default absolute difference 0.05)

This criterion targets a small absolute difference (default $\\delta=0.05$) between apparent and adjusted **Nagelkerke** ($R^2$). The required shrinkage implied by this constraint is:
$$
S_{\\delta}=\\frac{R^2_{CS}}{R^2_{CS}+\\delta R^2_{CS,\\max}}.
$$
Then:
$$
n_2=\\left\\lceil
\\frac{P}{(S_{\\delta}-1)\\ln\\left(1-\\frac{R^2_{CS}}{S_{\\delta}}\\right)}
\\right\\rceil.
$$

### Criterion 3 â€” Precise estimation of the overall outcome risk (intercept)

This targets precision of the **average outcome risk** ($p$) (baseline risk) within ($\\pm d$) on the probability scale (default $d=0.05$ at 95% CI):
$$
n_3=\\left\\lceil
\\left(\\frac{z_{1-\\alpha/2}}{d}\\right)^2 p(1-p)
\\right\\rceil,
\\quad \\text{default } z_{0.975}=1.96,; d=0.05.
$$

### Final recommendation

$$
n_{\\min}=\\max(n_1,n_2,n_3),\\qquad
E = n_{\\min}p,\\qquad
EPP=\\frac{E}{P}.
$$

---

## Practical guidance (typical choices)

* **Shrinkage (S)**: use **0.90** as a standard target; consider **0.95** if you want stronger overfitting control or if the model is complex.
* **$\\delta=0.05$** for Criterion 2: commonly kept at the default.
* **Intercept precision (d=0.05)**: default corresponds to estimating baseline risk within Â±5%. If baseline risk must be estimated more precisely, you would need a smaller ($d$) (larger ($n$)).
* **Anticipated ($R^2_{CS}$)**:

  * Prefer **optimism-adjusted** values from related studies (or apparent values from external validation data).
  * If only AUC is available, use the published AUCâ†’$R^2_{CS}$ approximation method.
  * If neither is available, the **15% of $R^2_{CS,\\max}$** option is a conservative fallback for exploratory planningâ€”always run sensitivity analyses.

---

## Key references (2â€“5)

1. Riley RD, Snell KIE, Ensor J, et al. *Minimum sample size required for developing a multivariable prediction model: PART IIâ€”binary and time-to-event outcomes.* Statistics in Medicine. 2019.
2. Riley RD, Ensor J, Snell KIE, et al. *Calculating the sample size required for developing a clinical prediction model.* BMJ. 2020.
3. Riley RD, Van Calster B, Collins GS. *A note on estimating the Coxâ€“Snell ($R^2$) from a reported C statistic (AUROC) to inform sample size calculations for developing a prediction model with a binary outcome.* Statistics in Medicine. 2021.
4. Harrell FE Jr, Lee KL, Mark DB. *Multivariable prognostic models: issues in developing models, evaluating assumptions and adequacy, and measuring and reducing errors.* Statistics in Medicine. 1996.
""",
}
